#!/usr/bin/env python3
"""
MiniMind æ•°æ®éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯é¢„è®­ç»ƒæ•°æ®çš„æ ¼å¼å’Œè´¨é‡
"""

import json
import os
from pathlib import Path
from collections import Counter
import re


class MiniMindDataValidator:
    """MiniMind æ•°æ®éªŒè¯å™¨"""
    
    def __init__(self, data_dir="dataset"):
        self.data_dir = Path(data_dir)
        
    def validate_pretrain_data(self, file_path="pretrain_data.jsonl"):
        """éªŒè¯é¢„è®­ç»ƒæ•°æ®æ ¼å¼"""
        full_path = self.data_dir / file_path
        
        if not full_path.exists():
            print(f"âŒ é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
            return False
        
        print(f"\n=== éªŒè¯é¢„è®­ç»ƒæ•°æ®: {file_path} ===")
        
        issues = []
        stats = {
            "total_samples": 0,
            "valid_samples": 0,
            "text_lengths": [],
            "char_counts": Counter(),
            "word_counts": Counter()
        }
        
        with open(full_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                stats["total_samples"] += 1
                
                try:
                    # è§£æJSON
                    data = json.loads(line.strip())
                    
                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                    if "text" not in data:
                        issues.append(f"ç¬¬{line_num}è¡Œ: ç¼ºå°‘ 'text' å­—æ®µ")
                        continue
                    
                    text = data["text"]
                    
                    # æ£€æŸ¥æ–‡æœ¬ç±»å‹
                    if not isinstance(text, str):
                        issues.append(f"ç¬¬{line_num}è¡Œ: 'text' å­—æ®µä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹")
                        continue
                    
                    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
                    if len(text.strip()) == 0:
                        issues.append(f"ç¬¬{line_num}è¡Œ: æ–‡æœ¬ä¸ºç©º")
                        continue
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    stats["valid_samples"] += 1
                    stats["text_lengths"].append(len(text))
                    
                    # å­—ç¬¦ç»Ÿè®¡
                    stats["char_counts"].update(text)
                    
                    # è¯é¢‘ç»Ÿè®¡ï¼ˆç®€å•åˆ†è¯ï¼‰
                    words = re.findall(r'\w+', text)
                    stats["word_counts"].update(words)
                    
                except json.JSONDecodeError as e:
                    issues.append(f"ç¬¬{line_num}è¡Œ: JSONè§£æé”™è¯¯ - {e}")
                except Exception as e:
                    issues.append(f"ç¬¬{line_num}è¡Œ: æœªçŸ¥é”™è¯¯ - {e}")
        
        # è¾“å‡ºéªŒè¯ç»“æœ
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {stats['valid_samples']}")
        print(f"  æ•°æ®è´¨é‡: {stats['valid_samples']/stats['total_samples']*100:.1f}%")
        
        if stats['valid_samples'] > 0:
            print(f"\nğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡é•¿åº¦: {sum(stats['text_lengths'])/len(stats['text_lengths']):.0f} å­—ç¬¦")
            print(f"  æœ€å°é•¿åº¦: {min(stats['text_lengths'])} å­—ç¬¦")
            print(f"  æœ€å¤§é•¿åº¦: {max(stats['text_lengths'])} å­—ç¬¦")
            
            print(f"\nğŸ”¤ å­—ç¬¦ç»Ÿè®¡ (å‰10):")
            for char, count in stats['char_counts'].most_common(10):
                print(f"  '{char}': {count} æ¬¡")
            
            print(f"\nğŸ“ è¯é¢‘ç»Ÿè®¡ (å‰10):")
            for word, count in stats['word_counts'].most_common(10):
                print(f"  '{word}': {count} æ¬¡")
        
        # è¾“å‡ºé—®é¢˜
        if issues:
            print(f"\nâš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
                print(f"  {issue}")
            if len(issues) > 10:
                print(f"  ... è¿˜æœ‰ {len(issues)-10} ä¸ªé—®é¢˜æœªæ˜¾ç¤º")
        else:
            print(f"\nâœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡!")
        
        return len(issues) == 0
    
    def validate_mini_dataset(self):
        """éªŒè¯å°è§„æ¨¡æ•°æ®é›†"""
        mini_path = self.data_dir / "mini_pretrain_data.jsonl"
        
        if not mini_path.exists():
            print(f"âŒ å°è§„æ¨¡æ•°æ®é›†ä¸å­˜åœ¨: {mini_path}")
            return False
        
        print(f"\n=== éªŒè¯å°è§„æ¨¡æ•°æ®é›† ===")
        return self.validate_pretrain_data("mini_pretrain_data.jsonl")
    
    def check_data_files(self):
        """æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("\n=== æ£€æŸ¥æ•°æ®æ–‡ä»¶ ===")
        
        files_to_check = [
            "pretrain_data.jsonl",
            "mini_pretrain_data.jsonl"
        ]
        
        all_exist = True
        for file_name in files_to_check:
            file_path = self.data_dir / file_name
            if file_path.exists():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                print(f"âœ… {file_name}: {size_mb:.1f} MB")
            else:
                print(f"âŒ {file_name}: æ–‡ä»¶ä¸å­˜åœ¨")
                all_exist = False
        
        return all_exist
    
    def run_full_validation(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®éªŒè¯"""
        print("ğŸš€ å¼€å§‹MiniMindæ•°æ®éªŒè¯...")
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        files_ok = self.check_data_files()
        
        # éªŒè¯ä¸»æ•°æ®é›†
        pretrain_ok = self.validate_pretrain_data()
        
        # éªŒè¯å°æ•°æ®é›†
        mini_ok = self.validate_mini_dataset()
        
        # æ€»ç»“
        print(f"\nğŸ“‹ éªŒè¯æ€»ç»“:")
        print(f"  æ–‡ä»¶æ£€æŸ¥: {'âœ… é€šè¿‡' if files_ok else 'âŒ å¤±è´¥'}")
        print(f"  é¢„è®­ç»ƒæ•°æ®: {'âœ… é€šè¿‡' if pretrain_ok else 'âŒ å¤±è´¥'}")
        print(f"  å°è§„æ¨¡æ•°æ®: {'âœ… é€šè¿‡' if mini_ok else 'âŒ å¤±è´¥'}")
        
        overall_success = files_ok and pretrain_ok and mini_ok
        if overall_success:
            print(f"\nğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡! æ•°æ®å‡†å¤‡å°±ç»ªã€‚")
        else:
            print(f"\nâš ï¸  éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
        
        return overall_success


def main():
    """ä¸»å‡½æ•°"""
    validator = MiniMindDataValidator()
    
    # è¿è¡Œå®Œæ•´éªŒè¯
    success = validator.run_full_validation()
    
    if success:
        print("\nâœ… æ•°æ®éªŒè¯å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ!")
        print("ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œåˆ†è¯å™¨è®­ç»ƒ: python scripts/train_tokenizer.py")
        print("2. å¼€å§‹é¢„è®­ç»ƒ: python trainer/train_pretrain.py")
    else:
        print("\nâŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·é‡æ–°å‡†å¤‡æ•°æ®ã€‚")
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())