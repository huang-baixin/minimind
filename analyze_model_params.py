#!/usr/bin/env python3
"""
åˆ†æMiniMindæ¨¡å‹å‚æ•°é‡
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model.model_minimind import MiniMindConfig, MiniMindModel
import torch

def calculate_model_params():
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    
    print("=== MiniMindæ¨¡å‹å‚æ•°é‡åˆ†æ ===\n")
    
    # é»˜è®¤é…ç½®
    config = MiniMindConfig()
    
    print("æ¨¡å‹é…ç½®å‚æ•°:")
    print(f"  - éšè—å±‚ç»´åº¦ (hidden_size): {config.hidden_size}")
    print(f"  - å±‚æ•° (num_hidden_layers): {config.num_hidden_layers}")
    print(f"  - æ³¨æ„åŠ›å¤´æ•° (num_attention_heads): {config.num_attention_heads}")
    print(f"  - KVå¤´æ•° (num_key_value_heads): {config.num_key_value_heads}")
    print(f"  - è¯æ±‡è¡¨å¤§å° (vocab_size): {config.vocab_size}")
    print(f"  - ä¸­é—´å±‚ç»´åº¦ (intermediate_size): {config.intermediate_size or config.hidden_size * 4}")
    
    # åˆ›å»ºæ¨¡å‹
    model = MiniMindModel(config)
    
    # è®¡ç®—æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"\næ€»å‚æ•°é‡: {total_params:,} ({total_params / 1e6:.2f}M)")
    
    # æŒ‰å±‚åˆ†è§£å‚æ•°
    print("\nå‚æ•°åˆ†è§£:")
    
    # åµŒå…¥å±‚å‚æ•°
    embedding_params = config.vocab_size * config.hidden_size
    print(f"  - åµŒå…¥å±‚: {embedding_params:,} ({embedding_params / total_params * 100:.1f}%)")
    
    # æ³¨æ„åŠ›å±‚å‚æ•°
    head_dim = config.hidden_size // config.num_attention_heads
    attention_params_per_layer = (
        config.hidden_size * config.hidden_size * 4  # QKVæŠ•å½± + è¾“å‡ºæŠ•å½±
    )
    attention_total = attention_params_per_layer * config.num_hidden_layers
    print(f"  - æ³¨æ„åŠ›å±‚: {attention_total:,} ({attention_total / total_params * 100:.1f}%)")
    
    # FFNå±‚å‚æ•°
    ffn_intermediate = config.intermediate_size or config.hidden_size * 4
    ffn_params_per_layer = (
        config.hidden_size * ffn_intermediate +  # ä¸ŠæŠ•å½±
        ffn_intermediate * config.hidden_size    # ä¸‹æŠ•å½±
    )
    ffn_total = ffn_params_per_layer * config.num_hidden_layers
    print(f"  - FFNå±‚: {ffn_total:,} ({ffn_total / total_params * 100:.1f}%)")
    
    # å½’ä¸€åŒ–å±‚å‚æ•°
    norm_params_per_layer = config.hidden_size * 2  # RMSNormæƒé‡
    norm_total = norm_params_per_layer * config.num_hidden_layers
    print(f"  - å½’ä¸€åŒ–å±‚: {norm_total:,} ({norm_total / total_params * 100:.1f}%)")
    
    # LM Headå‚æ•°ï¼ˆä¸åµŒå…¥å±‚å…±äº«ï¼‰
    lm_head_params = 0  # å…±äº«æƒé‡
    print(f"  - LM Head: {lm_head_params:,} (å…±äº«åµŒå…¥å±‚)")
    
    return total_params, config

def compare_with_other_models():
    """ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”"""
    
    print("\n=== ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯” ===\n")
    
    models = {
        "MiniMind (é»˜è®¤)": {"params": 25.8e6, "vocab": 6400, "layers": 8, "hidden": 512},
        "MiniMind2-Small": {"params": 26e6, "vocab": 6400, "layers": 8, "hidden": 512},
        "GPT-2 Small": {"params": 124e6, "vocab": 50257, "layers": 12, "hidden": 768},
        "GPT-2 Medium": {"params": 355e6, "vocab": 50257, "layers": 24, "hidden": 1024},
        "Qwen2-0.5B": {"params": 0.5e9, "vocab": 151936, "layers": 24, "hidden": 1024},
        "Qwen2-1.5B": {"params": 1.5e9, "vocab": 151936, "layers": 24, "hidden": 1536},
        "Qwen2-7B": {"params": 7e9, "vocab": 151936, "layers": 32, "hidden": 4096},
    }
    
    print("æ¨¡å‹åç§°           | å‚æ•°é‡    | è¯æ±‡è¡¨ | å±‚æ•° | éšè—ç»´åº¦")
    print("-" * 60)
    
    for name, info in models.items():
        params_str = f"{info['params']/1e6:.1f}M" if info['params'] < 1e9 else f"{info['params']/1e9:.1f}B"
        print(f"{name:<16} | {params_str:>8} | {info['vocab']:>6,} | {info['layers']:>4} | {info['hidden']:>8}")

def qwen_tokenizer_recommendation():
    """Qwen tokenizeræ¨è"""
    
    print("\n=== Qwen Tokenizeræ¨è ===\n")
    
    print("ğŸ“Š åŸºäºMiniMindæ¨¡å‹ç‰¹ç‚¹ï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹Qwen tokenizer:")
    
    recommendations = [
        {
            "æ¨¡å‹": "Qwen2-1.5B",
            "ç†ç”±": "è¯æ±‡è¡¨å¤§å°151,936ï¼Œä¸MiniMindçš„6,400ç›¸æ¯”æ›´ä¸°å¯Œï¼Œèƒ½æ›´å¥½å¤„ç†ä¸­æ–‡",
            "ä¼˜åŠ¿": "æ”¯æŒæ›´ç»†ç²’åº¦çš„ä¸­æ–‡åˆ†è¯ï¼Œè¯æ±‡è¦†ç›–æ›´å…¨é¢",
            "æ³¨æ„": "éœ€è¦è°ƒæ•´æ¨¡å‹é…ç½®ä»¥åŒ¹é…è¯æ±‡è¡¨å¤§å°"
        },
        {
            "æ¨¡å‹": "Qwen2-0.5B", 
            "ç†ç”±": "ç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ï¼Œè¯æ±‡è¡¨ç›¸åŒä½†æ¨¡å‹æ›´è½»é‡",
            "ä¼˜åŠ¿": "éƒ¨ç½²æˆæœ¬ä½ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ",
            "æ³¨æ„": "è¯æ±‡è¡¨è¾ƒå¤§ï¼Œå¯èƒ½å¢åŠ åµŒå…¥å±‚å‚æ•°"
        },
        {
            "æ¨¡å‹": "Qwen/Qwen2-7B-Chat",
            "ç†ç”±": "èŠå¤©ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¯¹è¯èƒ½åŠ›æ›´å¼º",
            "ä¼˜åŠ¿": "ç»è¿‡å¯¹è¯æ•°æ®è®­ç»ƒï¼Œå¯¹è¯æ•ˆæœæ›´å¥½",
            "æ³¨æ„": "æ¨¡å‹è¾ƒå¤§ï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æº"
        }
    ]
    
    for i, rec in enumerate(recommendations, 1):
        print(f"\n{i}. {rec['æ¨¡å‹']}:")
        print(f"   ğŸ“ {rec['ç†ç”±']}")
        print(f"   âœ… {rec['ä¼˜åŠ¿']}")
        print(f"   âš ï¸  {rec['æ³¨æ„']}")
    
    print("\nğŸ¯ æœ€ä½³æ¨è: Qwen2-1.5B")
    print("   - è¯æ±‡è¡¨ä¸°å¯Œï¼Œä¸­æ–‡æ”¯æŒå¥½")
    print("   - æ¨¡å‹å¤§å°é€‚ä¸­ï¼Œéƒ¨ç½²æˆæœ¬åˆç†")
    print("   - æ€§èƒ½ä¸èµ„æºæ¶ˆè€—å¹³è¡¡è‰¯å¥½")

def implementation_guide():
    """å®ç°æŒ‡å—"""
    
    print("\n=== å®ç°æŒ‡å— ===\n")
    
    print("1. ä½¿ç”¨Qwen tokenizerçš„æ­¥éª¤:")
    print("   - å®‰è£…ä¾èµ–: pip install transformers")
    print("   - åŠ è½½tokenizer: from transformers import AutoTokenizer")
    print("   - ä½¿ç”¨ä»£ç : tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-1.5B')")
    
    print("\n2. éœ€è¦è°ƒæ•´çš„é…ç½®:")
    print("   - ä¿®æ”¹MiniMindConfigä¸­çš„vocab_sizeä¸º151936")
    print("   - é‡æ–°åˆå§‹åŒ–åµŒå…¥å±‚æƒé‡")
    print("   - å¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹æ¶æ„ä»¥é€‚åº”æ›´å¤§çš„è¯æ±‡è¡¨")
    
    print("\n3. æ³¨æ„äº‹é¡¹:")
    print("   - åµŒå…¥å±‚å‚æ•°ä¼šæ˜¾è‘—å¢åŠ  (ä»3.3Må¢åŠ åˆ°78M)")
    print("   - éœ€è¦æ›´å¤šæ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´")
    print("   - ä½†èƒ½è·å¾—æ›´å¥½çš„ä¸­æ–‡å¤„ç†èƒ½åŠ›")

if __name__ == "__main__":
    # è®¡ç®—å‚æ•°é‡
    total_params, config = calculate_model_params()
    
    # å¯¹æ¯”å…¶ä»–æ¨¡å‹
    compare_with_other_models()
    
    # Qwen tokenizeræ¨è
    qwen_tokenizer_recommendation()
    
    # å®ç°æŒ‡å—
    implementation_guide()
    
    print("\n=== æ€»ç»“ ===")
    print(f"âœ… MiniMindé»˜è®¤é…ç½®å‚æ•°é‡: {total_params / 1e6:.1f}M")
    print("âœ… æ¨èä½¿ç”¨Qwen2-1.5Bçš„tokenizer")
    print("âœ… éœ€è¦è°ƒæ•´vocab_sizeé…ç½®ä»¥åŒ¹é…Qwen tokenizer")
    print("âœ… åµŒå…¥å±‚å‚æ•°ä¼šå¢åŠ ï¼Œä½†èƒ½è·å¾—æ›´å¥½çš„ä¸­æ–‡å¤„ç†èƒ½åŠ›")