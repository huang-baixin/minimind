# MiniMind åˆ†è¯å™¨è®­ç»ƒæµç¨‹åˆ†æ

## æ¦‚è¿°

MiniMindé¡¹ç›®ä½¿ç”¨è‡ªå®šä¹‰çš„BPEï¼ˆByte Pair Encodingï¼‰åˆ†è¯å™¨ï¼Œè¯¥åˆ†è¯å™¨åŸºäºHuggingFaceçš„`tokenizers`åº“ä»é›¶å¼€å§‹è®­ç»ƒã€‚åˆ†è¯å™¨è®­ç»ƒæ˜¯æ¨¡å‹è®­ç»ƒæµç¨‹çš„ç¬¬ä¸€æ­¥ï¼Œä¸ºåç»­çš„é¢„è®­ç»ƒå’Œå¾®è°ƒæä¾›åŸºç¡€æ”¯æŒã€‚

## åˆ†è¯å™¨è®­ç»ƒæµç¨‹å›¾

```mermaid
flowchart TD
    A[ğŸ“Š æ•°æ®å‡†å¤‡] --> B[ğŸ”¤ åˆ†è¯å™¨åˆå§‹åŒ–]
    B --> C[âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°]
    C --> D[ğŸ‹ï¸ è®­ç»ƒåˆ†è¯å™¨]
    D --> E[âœ… éªŒè¯ç‰¹æ®Štoken]
    E --> F[ğŸ’¾ ä¿å­˜åˆ†è¯å™¨æ–‡ä»¶]
    F --> G[âš™ï¸ åˆ›å»ºé…ç½®æ–‡ä»¶]
    G --> H[ğŸ§ª åˆ†è¯å™¨è¯„ä¼°]
    H --> I[âœ… è®­ç»ƒå®Œæˆ]
    
    subgraph "æ•°æ®å‡†å¤‡é˜¶æ®µ"
        A1[åŠ è½½pretrain_hq.jsonl] --> A2[æå–æ–‡æœ¬æ•°æ®]
        A2 --> A3[æ„å»ºæ–‡æœ¬è¿­ä»£å™¨]
    end
    
    subgraph "åˆ†è¯å™¨é…ç½®"
        B1[é€‰æ‹©BPEæ¨¡å‹] --> B2[è®¾ç½®ByteLevelé¢„åˆ†è¯]
        C1[å®šä¹‰ç‰¹æ®Štoken] --> C2[è®¾ç½®è¯æ±‡è¡¨å¤§å°6400]
        C2 --> C3[é…ç½®BPEè®­ç»ƒå™¨]
    end
    
    subgraph "æ–‡ä»¶ä¿å­˜"
        F1[ä¿å­˜tokenizer.json] --> F2[ä¿å­˜æ¨¡å‹æ–‡ä»¶]
        G1[åˆ›å»ºtokenizer_config.json] --> G2[é…ç½®èŠå¤©æ¨¡æ¿]
    end
    
    A --> A1
    B --> B1
    C --> C1
    F --> F1
    G --> G1
```

## è¯¦ç»†è®­ç»ƒæµç¨‹

### 1. æ•°æ®å‡†å¤‡é˜¶æ®µ

```python
def read_texts_from_jsonl(file_path):
    """ä»JSONLæ–‡ä»¶ä¸­æå–æ–‡æœ¬æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            yield data['text']
```

**æ•°æ®æ¥æº**: `../dataset/pretrain_hq.jsonl`
- åŒ…å«çº¦1.6GBçš„é«˜è´¨é‡é¢„è®­ç»ƒè¯­æ–™
- æ–‡æœ¬é•¿åº¦é™åˆ¶åœ¨512å­—ç¬¦ä»¥å†…
- æ•°æ®æ ¼å¼ä¸ºJSONLï¼Œæ¯è¡ŒåŒ…å«`text`å­—æ®µ

### 2. åˆ†è¯å™¨åˆå§‹åŒ–

```python
# åˆå§‹åŒ–BPEæ¨¡å‹çš„åˆ†è¯å™¨
tokenizer = Tokenizer(models.BPE())

# è®¾ç½®ByteLevelé¢„åˆ†è¯å™¨
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

**é…ç½®è¯´æ˜**:
- **æ¨¡å‹ç±»å‹**: BPE (Byte Pair Encoding)
- **é¢„åˆ†è¯å™¨**: ByteLevelï¼Œå¤„ç†Unicodeå­—ç¬¦
- **å‰ç¼€ç©ºæ ¼**: ä¸æ·»åŠ å‰ç¼€ç©ºæ ¼

### 3. è®­ç»ƒå‚æ•°é…ç½®

```python
special_tokens = ["<|endoftext|>", "<|im_start|>", "<|im_end|>"]

trainer = trainers.BpeTrainer(
    vocab_size=6400,           # è¯æ±‡è¡¨å¤§å°
    special_tokens=special_tokens,  # ç‰¹æ®Štoken
    show_progress=True,        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # åˆå§‹å­—æ¯è¡¨
)
```

**å…³é”®å‚æ•°**:
- **è¯æ±‡è¡¨å¤§å°**: 6400ä¸ªtokenï¼Œé€‚åˆå°å‹æ¨¡å‹
- **ç‰¹æ®Štoken**: 3ä¸ªé¢„å®šä¹‰çš„ç‰¹æ®Šæ ‡è®°
- **å­—æ¯è¡¨**: åŸºäºByteLevelçš„Unicodeæ”¯æŒ

### 4. è®­ç»ƒè¿‡ç¨‹

```python
# è¯»å–æ–‡æœ¬æ•°æ®
texts = read_texts_from_jsonl(data_path)

# è®­ç»ƒåˆ†è¯å™¨
tokenizer.train_from_iterator(texts, trainer=trainer)

# è®¾ç½®ByteLevelè§£ç å™¨
tokenizer.decoder = decoders.ByteLevel()
```

**è®­ç»ƒç‰¹ç‚¹**:
- ä½¿ç”¨è¿­ä»£å™¨æ–¹å¼å¤„ç†å¤§æ•°æ®é›†
- æ”¯æŒæµå¼è®­ç»ƒï¼Œå†…å­˜å‹å¥½
- è‡ªåŠ¨å­¦ä¹ BPEåˆå¹¶è§„åˆ™

### 5. ç‰¹æ®ŠtokenéªŒè¯

```python
# éªŒè¯ç‰¹æ®Štokençš„ç´¢å¼•
assert tokenizer.token_to_id("<|endoftext|>") == 0
assert tokenizer.token_to_id("<|im_start|>") == 1
assert tokenizer.token_to_id("<|im_end|>") == 2
```

**ç‰¹æ®ŠtokenåŠŸèƒ½**:
- `<|endoftext|>` (ç´¢å¼•0): æ–‡æœ¬ç»“æŸæ ‡è®°ï¼ŒåŒæ—¶ä½œä¸ºpad_tokenå’Œunk_token
- `<|im_start|>` (ç´¢å¼•1): å¯¹è¯å¼€å§‹æ ‡è®°ï¼Œä½œä¸ºbos_token
- `<|im_end|>` (ç´¢å¼•2): å¯¹è¯ç»“æŸæ ‡è®°ï¼Œä½œä¸ºeos_token

### 6. æ–‡ä»¶ä¿å­˜

```python
# ä¿å­˜åˆ†è¯å™¨æ–‡ä»¶
tokenizer.save(os.path.join(tokenizer_dir, "tokenizer.json"))
tokenizer.model.save("../model/")

# æ‰‹åŠ¨åˆ›å»ºé…ç½®æ–‡ä»¶
config = {
    "add_bos_token": False,
    "add_eos_token": False,
    "bos_token": "<|im_start|>",
    "eos_token": "<|im_end|>",
    "pad_token": "<|endoftext|>",
    "unk_token": "<|endoftext|>",
    "model_max_length": 32768,
    # ... å®Œæ•´é…ç½®
}
```

**ç”Ÿæˆçš„æ–‡ä»¶**:
- `tokenizer.json`: åˆ†è¯å™¨çš„ä¸»è¦é…ç½®æ–‡ä»¶
- `tokenizer_config.json`: HuggingFaceå…¼å®¹çš„é…ç½®æ–‡ä»¶
- æ¨¡å‹ç›¸å…³æ–‡ä»¶

### 7. èŠå¤©æ¨¡æ¿é…ç½®

åˆ†è¯å™¨é…ç½®åŒ…å«å¤æ‚çš„èŠå¤©æ¨¡æ¿ï¼Œæ”¯æŒï¼š
- å¤šè½®å¯¹è¯å¤„ç†
- å·¥å…·è°ƒç”¨åŠŸèƒ½ (`<tool_call>`æ ‡ç­¾)
- æ¨ç†æ€è€ƒé“¾ (`<thinking>`æ ‡ç­¾)
- ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯åŒºåˆ†

## åˆ†è¯å™¨æŠ€æœ¯ç‰¹ç‚¹

### 1. BPEç®—æ³•ä¼˜åŠ¿
```mermaid
graph LR
    A[åŸå§‹æ–‡æœ¬] --> B[ByteLevelç¼–ç ]
    B --> C[BPEåˆå¹¶]
    C --> D[è¯æ±‡è¡¨æ„å»º]
    D --> E[é«˜æ•ˆåˆ†è¯]
```

- **å­è¯çº§åˆ«**: å¤„ç†æœªçŸ¥è¯æ±‡èƒ½åŠ›å¼º
- **å¤šè¯­è¨€æ”¯æŒ**: ByteLevelå¤„ç†æ‰€æœ‰Unicodeå­—ç¬¦
- **å‹ç¼©æ•ˆç‡**: 6400è¯æ±‡è¡¨å¤§å°å¹³è¡¡äº†æ•ˆç‡å’Œæ€§èƒ½

### 2. ç‰¹æ®Štokenè®¾è®¡

| Token | ç´¢å¼• | åŠŸèƒ½ | ç”¨é€” |
|-------|------|------|------|
| `<|endoftext|>` | 0 | æ–‡æœ¬ç»“æŸ/Pad/Unknown | å¡«å……ã€æœªçŸ¥è¯å¤„ç† |
| `<|im_start|>` | 1 | å¯¹è¯å¼€å§‹ | æ ‡è®°å¯¹è¯å¼€å§‹ |
| `<|im_end|>` | 2 | å¯¹è¯ç»“æŸ | æ ‡è®°å¯¹è¯ç»“æŸ |

### 3. èŠå¤©æ¨¡æ¿ç‰¹æ€§

```python
# æ”¯æŒçš„å·¥å…·è°ƒç”¨æ ¼å¼
<tool_call>
{"name": "function_name", "arguments": {}}
</tool_call>

# æ”¯æŒçš„æ€è€ƒé“¾æ ¼å¼
<thinking>
æ¨ç†è¿‡ç¨‹...
</thinking>
<im_end>

# å¤šè½®å¯¹è¯æ”¯æŒ
<|im_start|>system
ç³»ç»Ÿæ¶ˆæ¯<|im_end|>
<|im_start|>user
ç”¨æˆ·æ¶ˆæ¯<|im_end|>
<|im_start|>assistant
åŠ©æ‰‹å›å¤<|im_end|>
```

## è®­ç»ƒæµç¨‹ä¾èµ–å…³ç³»

```mermaid
graph TB
    A[ğŸ“Š pretrain_hq.jsonl] --> B[ğŸ”¤ train_tokenizer.py]
    B --> C[ğŸ’¾ tokenizer.json]
    B --> D[âš™ï¸ tokenizer_config.json]
    
    C --> E[ğŸ‹ï¸ é¢„è®­ç»ƒ]
    D --> E
    
    E --> F[ğŸ¯ ç›‘ç£å¾®è°ƒ]
    F --> G[â¤ï¸ DPOè®­ç»ƒ]
    F --> H[âš¡ LoRAå¾®è°ƒ]
    F --> I[ğŸ§  æ¨¡å‹è’¸é¦]
    
    subgraph "æ¨¡å‹è®­ç»ƒæµç¨‹"
        E --> F --> G --> H --> I
    end
    
    subgraph "åˆ†è¯å™¨åº”ç”¨"
        J[ğŸ“ æ–‡æœ¬ç¼–ç ] --> K[ğŸ’¬ å¯¹è¯ç”Ÿæˆ]
        K --> L[ğŸ› ï¸ å·¥å…·è°ƒç”¨]
        K --> M[ğŸ¤” æ¨ç†æ€è€ƒ]
    end
    
    D --> J
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. è®­ç»ƒåˆ†è¯å™¨
```bash
cd scripts/
python train_tokenizer.py
```

### 2. åŠ è½½å’Œä½¿ç”¨åˆ†è¯å™¨
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("../model/")

# ç¼–ç æ–‡æœ¬
encoded = tokenizer("ä½ å¥½ï¼Œä¸–ç•Œï¼")
print(encoded.input_ids)

# åº”ç”¨èŠå¤©æ¨¡æ¿
messages = [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ"}
]
chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False)
print(chat_prompt)
```

## æ€§èƒ½ç‰¹ç‚¹

1. **é«˜æ•ˆæ€§**: ByteLevelé¢„åˆ†è¯ + BPEåˆå¹¶ï¼Œå¤„ç†é€Ÿåº¦å¿«
2. **å…¼å®¹æ€§**: å®Œå…¨å…¼å®¹HuggingFace Transformersç”Ÿæ€
3. **çµæ´»æ€§**: æ”¯æŒå¤šç§å¯¹è¯æ ¼å¼å’Œå·¥å…·è°ƒç”¨
4. **è½»é‡åŒ–**: 6400è¯æ±‡è¡¨ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ

è¿™ä¸ªåˆ†è¯å™¨è®­ç»ƒæµç¨‹ä¸ºMiniMindé¡¹ç›®çš„æ•´ä¸ªè®­ç»ƒæµç¨‹æä¾›äº†åšå®çš„åŸºç¡€ï¼Œç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®å¤„ç†ä¸­æ–‡æ–‡æœ¬å’Œå¤æ‚çš„å¯¹è¯åœºæ™¯ã€‚