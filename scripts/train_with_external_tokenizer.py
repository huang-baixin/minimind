#!/usr/bin/env python3
"""
ä½¿ç”¨å¤–éƒ¨tokenizerçš„è®­ç»ƒè„šæœ¬
æ”¯æŒä»Hugging Face HubåŠ è½½é¢„è®­ç»ƒtokenizerï¼Œé¿å…æœ¬åœ°è®­ç»ƒ
"""

import os
import sys
from transformers import AutoTokenizer, PreTrainedTokenizerFast


def setup_external_tokenizer(tokenizer_name="Qwen/Qwen2.5-0.5B", save_dir="../model/external_tokenizer"):
    """
    è®¾ç½®å¤–éƒ¨tokenizer
    
    Args:
        tokenizer_name: Hugging Faceæ¨¡å‹åç§°
        save_dir: ä¿å­˜tokenizeræ–‡ä»¶çš„ç›®å½•
    
    Returns:
        tokenizer: é…ç½®å¥½çš„tokenizerå¯¹è±¡
        vocab_size: è¯æ±‡è¡¨å¤§å°
    """
    
    print(f"ğŸš€ æ­£åœ¨è®¾ç½®å¤–éƒ¨tokenizer: {tokenizer_name}")
    
    try:
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)
        
        # ä»Hugging FaceåŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_name,
            trust_remote_code=True,
            local_files_only=False
        )
        
        print(f"âœ… æˆåŠŸåŠ è½½tokenizer: {tokenizer_name}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        
        # é€‚é…MiniMindçš„ç‰¹æ®Štokené…ç½®
        tokenizer = adapt_tokenizer_for_minimind(tokenizer)
        
        # ä¿å­˜tokenizeræ–‡ä»¶
        tokenizer.save_pretrained(save_dir)
        print(f"âœ… tokenizeræ–‡ä»¶å·²ä¿å­˜åˆ°: {save_dir}")
        
        # åˆ›å»ºtokenizer_config.jsonï¼ˆé€‚é…MiniMindé…ç½®ï¼‰
        create_minimind_tokenizer_config(tokenizer, save_dir)
        
        return tokenizer, tokenizer.vocab_size
        
    except Exception as e:
        print(f"âŒ è®¾ç½®å¤–éƒ¨tokenizerå¤±è´¥: {e}")
        return None, None


def adapt_tokenizer_for_minimind(tokenizer):
    """é€‚é…tokenizerä»¥å…¼å®¹MiniMindé¡¹ç›®çš„ç‰¹æ®Štokené…ç½®"""
    
    print("ğŸ”„ é€‚é…tokenizerä»¥å…¼å®¹MiniMindé…ç½®...")
    
    # MiniMindé¡¹ç›®çš„ç‰¹æ®Štoken
    minimind_tokens = {
        "bos_token": "<|im_start|>",
        "eos_token": "<|im_end|>",
        "pad_token": "<|endoftext|>",
        "unk_token": "<|endoftext|>"
    }
    
    # æ£€æŸ¥å¹¶æ·»åŠ ç‰¹æ®Štoken
    for token_name, token_value in minimind_tokens.items():
        current_token = getattr(tokenizer, token_name, None)
        
        if current_token != token_value:
            # å¦‚æœtokenä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œéœ€è¦æ·»åŠ 
            if tokenizer.convert_tokens_to_ids(token_value) == tokenizer.unk_token_id:
                # æ·»åŠ æ–°token
                tokenizer.add_tokens([token_value], special_tokens=True)
                print(f"   æ·»åŠ ç‰¹æ®Štoken: {token_value}")
            
            # æ›´æ–°tokenizeré…ç½®
            setattr(tokenizer, token_name, token_value)
            setattr(tokenizer, f"{token_name}_id", tokenizer.convert_tokens_to_ids(token_value))
            print(f"   è®¾ç½®{token_name}: {token_value} (ID: {getattr(tokenizer, f'{token_name}_id')})")
    
    return tokenizer


def create_minimind_tokenizer_config(tokenizer, save_dir):
    """åˆ›å»ºé€‚é…MiniMindçš„tokenizeré…ç½®æ–‡ä»¶"""
    
    config = {
        "add_bos_token": False,
        "add_eos_token": False,
        "add_prefix_space": False,
        "bos_token": getattr(tokenizer, 'bos_token', '<|im_start|>'),
        "eos_token": getattr(tokenizer, 'eos_token', '<|im_end|>'),
        "pad_token": getattr(tokenizer, 'pad_token', '<|endoftext|>'),
        "unk_token": getattr(tokenizer, 'unk_token', '<|endoftext|>'),
        "model_max_length": 32768,
        "tokenizer_class": "PreTrainedTokenizerFast",
        "chat_template": """{%- if tools %}
    {{- '<|im_start|>system\\n' }}
    {%- if messages[0].role == 'system' %}
        {{- messages[0].content + '\\n\\n' }}
    {%- endif %}
    {{- "# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>" }}
    {%- for tool in tools %}
        {{- "\\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\"name\\": <function-name>, \\"arguments\\": <args-json-object>}\\n</tool_call><|im_end|>\\n" }}
{%- else %}
 {%- if messages[0]['role'] == 'system' -%}
        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}
    {%- else -%}
        {{- '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}
 {%- endif %}
{%- endif %}
{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
{%- for message in messages[::-1] %}
    {%- set index = (messages|length - 1) - loop.index0 %}
    {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
        {%- set ns.multi_step_tool = false %}
        {%- set ns.last_query_index = index %}
    {%- endif %}
{%- endfor %}
{%- for message in messages %}
    {%- if message.content is string %}
        {%- set content = message.content %}
    {%- else %}
        {%- set content = '' %}
    {%- endif %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}
    {%- elif message.role == "assistant" %}
   {{- '<|im_start|>' + message.role + '\\n' + content }}
  {%- if message.tool_calls %}
            {%- for tool_call in message.tool_calls %}
                {%- if (loop.first and content) or (not loop.first) %}
                    {{- '\\n' }}
                {%- endif %}
                {%- if tool_call.function %}
                    {%- set tool_call = tool_call.function %}
                {%- endif %}
                {{- '<tool_call>\\n{\"name\": \"' }}
                {{- tool_call.name }}
                {{- '\", \"arguments\": ' }}
                {%- if tool_call.arguments is string %}
                    {{- tool_call.arguments }}
                {%- else %}
                    {{- tool_call.arguments | tojson }}
                {%- endif %}
                {{- '}\\n</tool_call>' }}
            {%- endfor %}
        {%- endif %}
        {{- '<|im_end|>\\n' }}
    {%- elif message.role == "tool" %}
        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
            {{- '<|im_start|>user' }}
        {%- endif %}
        {{- '\\n<tool_response>\\n' }}
        {{- content }}
        {{- '\\n</tool_response>' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '<|im_end|>\\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|im_start|>assistant\\n' }}
    {%- if enable_thinking is defined and enable_thinking is false %}
        {{- 'ğŸ› ï¸\\n\\nğŸ”§\\n\\n' }}
    {%- endif %}
{%- endif %}"""
    }
    
    import json
    config_path = os.path.join(save_dir, "tokenizer_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… åˆ›å»ºtokenizeré…ç½®æ–‡ä»¶: {config_path}")


def validate_tokenizer(tokenizer, test_texts=None):
    """éªŒè¯tokenizeråŠŸèƒ½"""
    
    if test_texts is None:
        test_texts = [
            "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•",
            "Hello, this is a test",
            "æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ",
            "Natural language processing"
        ]
    
    print("\nğŸ§ª éªŒè¯tokenizeråŠŸèƒ½:")
    
    for text in test_texts:
        # ç¼–ç æµ‹è¯•
        encoded = tokenizer.encode(text, add_special_tokens=False)
        decoded = tokenizer.decode(encoded)
        
        print(f"  æ–‡æœ¬: {text[:30]}...")
        print(f"  Tokenæ•°é‡: {len(encoded)}")
        print(f"  è§£ç ç»“æœ: {decoded[:30]}...")
        
        # æ£€æŸ¥ç‰¹æ®Štoken
        if hasattr(tokenizer, 'bos_token_id'):
            print(f"  BOS Token ID: {tokenizer.bos_token_id}")
        if hasattr(tokenizer, 'eos_token_id'):
            print(f"  EOS Token ID: {tokenizer.eos_token_id}")
        
        print("  ---")


def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ MiniMindé¡¹ç›® - ä½¿ç”¨å¤–éƒ¨tokenizerè®­ç»ƒ")
    print("=" * 50)
    
    # å¯é€‰çš„tokenizeråˆ—è¡¨
    tokenizer_options = [
        "Qwen/Qwen2.5-0.5B",      # æ¨èï¼šä¸­æ–‡ä¼˜åŒ–ï¼Œè¯æ±‡è¡¨åˆç†
        "Qwen/Qwen2.5-1.5B",      # ä¸­ç­‰è§„æ¨¡
        "THUDM/chatglm3-6b",      # ChatGLM tokenizer
        "baichuan-inc/Baichuan2-7B-Base",  # Baichuan tokenizer
        "meta-llama/Llama-3.2-1B" # Llama tokenizerï¼ˆè‹±æ–‡ä¼˜åŒ–ï¼‰
    ]
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªé€‰é¡¹ï¼ˆQwen 0.5Bï¼Œæ¨èç”¨äºä¸­æ–‡ï¼‰
    selected_tokenizer = tokenizer_options[0]
    
    # è®¾ç½®å¤–éƒ¨tokenizer
    tokenizer, vocab_size = setup_external_tokenizer(selected_tokenizer)
    
    if tokenizer is None:
        print("âŒ tokenizerè®¾ç½®å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # éªŒè¯tokenizer
    validate_tokenizer(tokenizer)
    
    print("\nâœ… å¤–éƒ¨tokenizerè®¾ç½®å®Œæˆï¼")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("1. ä¿®æ”¹æ¨¡å‹é…ç½®ä¸­çš„vocab_sizeä¸º: {}".format(vocab_size))
    print("2. åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨æ–°çš„tokenizerè·¯å¾„")
    print("3. å¼€å§‹æ¨¡å‹è®­ç»ƒ")
    print("\nğŸ’¡ æç¤º: å¤–éƒ¨tokenizerå·²ç»è¿‡å¤§è§„æ¨¡æ•°æ®è®­ç»ƒï¼Œé€šå¸¸æ¯”æœ¬åœ°è®­ç»ƒçš„æ•ˆæœæ›´å¥½")


if __name__ == "__main__":
    main()