#!/usr/bin/env python3
"""
åˆ†æä¸åŒtokenizerçš„æ–‡ä»¶ç»“æ„å’Œä½¿ç”¨æ–¹æ³•
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from transformers import AutoTokenizer
import json

def analyze_minimind_tokenizer():
    """åˆ†æMiniMind tokenizerçš„æ–‡ä»¶ç»“æ„"""
    
    print("=== MiniMind Tokenizeræ–‡ä»¶åˆ†æ ===\n")
    
    # åŠ è½½MiniMind tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained("./model")
        print("âœ… MiniMind tokenizeråŠ è½½æˆåŠŸ")
        
        # åˆ†ææ–‡ä»¶ç»“æ„
        model_dir = "./model"
        files = os.listdir(model_dir)
        tokenizer_files = [f for f in files if 'tokenizer' in f]
        
        print("æ–‡ä»¶ç»“æ„:")
        for file in tokenizer_files:
            file_path = os.path.join(model_dir, file)
            file_size = os.path.getsize(file_path)
            print(f"  - {file} ({file_size:,} bytes)")
            
            # ç®€è¦æŸ¥çœ‹æ–‡ä»¶å†…å®¹
            if file == "tokenizer.json":
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"    - åŒ…å«: vocabå­—å…¸ + BPEæ¨¡å‹ + é¢„å¤„ç†é…ç½®")
                    print(f"    - vocabå¤§å°: {len(data.get('model', {}).get('vocab', {}))}")
            elif file == "tokenizer_config.json":
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"    - åŒ…å«: ç‰¹æ®Štokené…ç½® + æ¨¡å‹å‚æ•°")
                    print(f"    - ç‰¹æ®Štoken: {list(data.get('added_tokens_decoder', {}).keys())}")
        
        return True
    except Exception as e:
        print(f"âŒ MiniMind tokenizeråŠ è½½å¤±è´¥: {e}")
        return False

def analyze_qwen_tokenizer():
    """åˆ†æQwen tokenizerçš„æ–‡ä»¶ç»“æ„"""
    
    print("\n=== Qwen Tokenizeræ–‡ä»¶åˆ†æ ===\n")
    
    try:
        # å°è¯•ä»Hugging FaceåŠ è½½Qwen tokenizer
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B", trust_remote_code=True)
        print("âœ… Qwen tokenizeråŠ è½½æˆåŠŸ")
        
        # è·å–tokenizerçš„é…ç½®ä¿¡æ¯
        print("Qwen tokenizerç‰¹ç‚¹:")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
        print(f"  - ç‰¹æ®Štokenæ•°é‡: {len(tokenizer.special_tokens_map)}")
        print(f"  - æ¨¡å‹æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")
        
        # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œåˆ†ææ–‡ä»¶ç»“æ„
        print("\nQwen tokenizeré€šå¸¸åŒ…å«çš„æ–‡ä»¶:")
        print("  - tokenizer.json (ä¸€ä½“åŒ–æ ¼å¼)")
        print("  - tokenizer_config.json")
        print("  - special_tokens_map.json")
        
        return True
    except Exception as e:
        print(f"âŒ Qwen tokenizeråŠ è½½å¤±è´¥: {e}")
        return False

def analyze_gpt2_tokenizer():
    """åˆ†æGPT-2 tokenizerçš„æ–‡ä»¶ç»“æ„ï¼ˆä¼ ç»Ÿæ ¼å¼ï¼‰"""
    
    print("\n=== GPT-2 Tokenizeræ–‡ä»¶åˆ†æï¼ˆä¼ ç»Ÿæ ¼å¼ï¼‰ ===\n")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        print("âœ… GPT-2 tokenizeråŠ è½½æˆåŠŸ")
        
        print("GPT-2 tokenizeræ–‡ä»¶ç»“æ„ï¼ˆä¼ ç»Ÿæ ¼å¼ï¼‰:")
        print("  - vocab.json (è¯æ±‡è¡¨æ˜ å°„)")
        print("  - merges.txt (BPEåˆå¹¶è§„åˆ™)")
        print("  - tokenizer_config.json")
        print("  - special_tokens_map.json")
        
        print(f"\nGPT-2 tokenizerç‰¹ç‚¹:")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
        print(f"  - ä½¿ç”¨BPEç®—æ³•")
        print(f"  - éœ€è¦å¤šä¸ªæ–‡ä»¶é…åˆä½¿ç”¨")
        
        return True
    except Exception as e:
        print(f"âŒ GPT-2 tokenizeråŠ è½½å¤±è´¥: {e}")
        return False

def demonstrate_tokenizer_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒtokenizer"""
    
    print("\n=== Tokenizerä½¿ç”¨æ¼”ç¤º ===\n")
    
    test_text = "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯"
    
    # 1. ä½¿ç”¨MiniMind tokenizer
    print("1. MiniMind tokenizer:")
    try:
        minimind_tokenizer = AutoTokenizer.from_pretrained("./model")
        minimind_ids = minimind_tokenizer.encode(test_text)
        print(f"   - è¾“å…¥: '{test_text}'")
        print(f"   - Token IDs: {minimind_ids}")
        print(f"   - Tokenæ•°é‡: {len(minimind_ids)}")
        print(f"   - è§£ç : '{minimind_tokenizer.decode(minimind_ids)}'")
    except Exception as e:
        print(f"   - å¤±è´¥: {e}")
    
    # 2. ä½¿ç”¨GPT-2 tokenizer
    print("\n2. GPT-2 tokenizer:")
    try:
        gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
        gpt2_ids = gpt2_tokenizer.encode(test_text)
        print(f"   - è¾“å…¥: '{test_text}'")
        print(f"   - Token IDs: {gpt2_ids}")
        print(f"   - Tokenæ•°é‡: {len(gpt2_ids)}")
        print(f"   - è§£ç : '{gpt2_tokenizer.decode(gpt2_ids)}'")
    except Exception as e:
        print(f"   - å¤±è´¥: {e}")

def create_tokenizer_migration_guide():
    """åˆ›å»ºtokenizerè¿ç§»æŒ‡å—"""
    
    print("\n=== Tokenizerè¿ç§»æŒ‡å— ===\n")
    
    print("æƒ…å†µ1: ä½¿ç”¨ä¸€ä½“åŒ–æ ¼å¼tokenizerï¼ˆå¦‚MiniMindã€Qwenï¼‰")
    print("  âœ… åªéœ€è¦: tokenizer.json + tokenizer_config.json")
    print("  ğŸ“ æ–‡ä»¶ç»“æ„:")
    print("    tokenizer/")
    print("    â”œâ”€â”€ tokenizer.json      # æ ¸å¿ƒæ–‡ä»¶ï¼ˆå¿…é¡»ï¼‰")
    print("    â””â”€â”€ tokenizer_config.json  # é…ç½®æ–‡ä»¶ï¼ˆå¿…é¡»ï¼‰")
    
    print("\næƒ…å†µ2: ä½¿ç”¨ä¼ ç»Ÿæ ¼å¼tokenizerï¼ˆå¦‚GPT-2ã€BERTï¼‰")
    print("  âœ… éœ€è¦: vocab.json + merges.txt + tokenizer_config.json")
    print("  ğŸ“ æ–‡ä»¶ç»“æ„:")
    print("    tokenizer/")
    print("    â”œâ”€â”€ vocab.json          # è¯æ±‡è¡¨ï¼ˆå¿…é¡»ï¼‰")
    print("    â”œâ”€â”€ merges.txt          # BPEè§„åˆ™ï¼ˆå¿…é¡»ï¼Œå¦‚æœä½¿ç”¨BPEï¼‰")
    print("    â”œâ”€â”€ tokenizer_config.json  # é…ç½®ï¼ˆå¿…é¡»ï¼‰")
    print("    â””â”€â”€ special_tokens_map.json  # ç‰¹æ®Štokenï¼ˆå¯é€‰ï¼‰")
    
    print("\nå…³é”®æ£€æŸ¥ç‚¹:")
    print("  1. ç¡®è®¤tokenizerç±»å‹ï¼ˆBPEã€WordPieceã€SentencePieceç­‰ï¼‰")
    print("  2. æ£€æŸ¥è¯æ±‡è¡¨å¤§å°æ˜¯å¦ä¸æ¨¡å‹åŒ¹é…")
    print("  3. éªŒè¯ç‰¹æ®Štokené…ç½®æ˜¯å¦æ­£ç¡®")
    print("  4. æµ‹è¯•tokenizeræ˜¯å¦èƒ½æ­£å¸¸ç¼–ç /è§£ç ")

if __name__ == "__main__":
    # è¿è¡Œåˆ†æ
    analyze_minimind_tokenizer()
    analyze_qwen_tokenizer()
    analyze_gpt2_tokenizer()
    
    # æ¼”ç¤ºä½¿ç”¨
    demonstrate_tokenizer_usage()
    
    # åˆ›å»ºè¿ç§»æŒ‡å—
    create_tokenizer_migration_guide()
    
    print("\n=== æ€»ç»“ ===")
    print("âœ… ä¸€ä½“åŒ–æ ¼å¼tokenizer: åªéœ€è¦tokenizer.json + tokenizer_config.json")
    print("âœ… ä¼ ç»Ÿæ ¼å¼tokenizer: éœ€è¦vocab.json + merges.txt + tokenizer_config.json")
    print("ğŸ” å…³é”®: æ£€æŸ¥tokenizerçš„å®é™…æ–‡ä»¶ç»“æ„ï¼Œç¡®ä¿æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å­˜åœ¨")